{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression (การไล่ระดับสำหรับการถดถอยเชิงเส้น) (LAB ไม่บังคับ)\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images/C1_W1_L4_S1_Lecture_GD.png\"  style=\"width:800px;height:200px;\" ></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## เป้าหมาย\n",
    "ในแล็บนี้ คุณจะ:\n",
    "- อัตโนมัติการปรับปรุงค่า w และ b โดยใช้วิธีการไล่ระดับ (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## เครื่องมือ (Tools)\n",
    "ในแล็บนี้ เราจะใช้เครื่องมือต่างๆ ดังนี้:\n",
    "- NumPy: ไลบรารียอดนิยมสำหรับการคำนวณทางวิทยาศาสตร์\n",
    "- Matplotlib: ไลบรารียอดนิยมสำหรับการพล็อตข้อมูล\n",
    "- ฟังก์ชันการพล็อตข้อมูลในไฟล์ lab_utils.py: ซึ่งเป็นไฟล์ที่อยู่ในไดเรกทอรี่ปัจจุบัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2\"></a>\n",
    "## ปัญหา\n",
    "ให้ใช้ข้อมูลสองชุดเดียวกันกับก่อนหน้านี้  ซึ่งเป็นข้อมูลเกี่ยวกับบ้านสองหลัง:\n",
    "\n",
    "- บ้านหลังแรก มีขนาด 1,000 ตารางฟุต ขายได้ในราคา $300,000\n",
    "- บ้านหลังที่สอง มีขนาด 2,000 ตารางฟุต ขายได้ในราคา $500,000\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.0.1\"></a>\n",
    "### Compute_Cost\n",
    "ฟังก์ชันนี้ถูกพัฒนาในห้องปฏิบัติการที่แล้ว เราจะต้องใช้มันอีกครั้งที่นี่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.1\"></a>\n",
    "## สรุปการไล่ระดับ (Gradient descent)\n",
    "จากบทเรียนที่ผ่านมา คุณได้พัฒนาโมเดลเชิงเส้นเพื่อทำนายค่า  $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "ใน regression เชิงเส้น คุณใช้ข้อมูลการฝึก (training data) เพื่อปรับพารามิเตอร์ $w$,$b$ โดยการลดค่าความผิดพลาดระหว่างค่าที่ทำนายได้ $f_{w,b}(x^{(i)})$ กับค่าจริง $y^{(i)}$. ตัวชี้วัดความผิดพลาดนี้ เรียกว่า cost หรือ $J(w,b)$. ในการฝึกโมเดล คุณจะวัดค่า cost โดยพิจารณาจากตัวอย่างการฝึกทั้งหมด $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ในบทเรียน Gradient Descent ได้อธิบายไว้ดังนี้:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "โดยที่:\n",
    "- พารามิเตอร์ w และ b จะถูกอัปเดต พร้อมกัน\n",
    "- การคำนวณอนุพันธ์ย่อย (partial derivative) ของ w และ b จะถูกคำนวณก่อนที่จะอัปเดตพารามิเตอร์ใดๆ\n",
    "\n",
    "นิยามของ Gradient:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ความหมายของ *\"simultaneously\"*:\n",
    "\n",
    "หมายถึงการคำนวณอนุพันธ์ย่อยของพารามิเตอร์ทั้งหมดก่อนที่จะอัปเดตพารามิเตอร์ใดๆ\n",
    "ไม่ใช่การอัปเดตพารามิเตอร์ทีละตัว\n",
    "การอัปเดตแบบพร้อมกันช่วยให้การปรับปรุงโมเดลมีประสิทธิภาพมากขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.2\"></a>\n",
    "## ใช้อัลกอริทึม Gradient Descent\n",
    "คุณจะต้องเขียนฟังก์ชัน 3 ฟังก์ชันเพื่อใช้อัลกอริทึม Gradient Descent สำหรับคุณสมบัติ (feature) เดียว\n",
    "- `compute_gradient` ใช้อสมการ (4) และ (5) ด้านบน\n",
    "- `compute_cost` ใช้อสมการ (2) ด้านบน (จาก Lab ก่อนหน้า)\n",
    "- `gradient_descent` ช้อสมการ compute_gradient และ compute_cost\n",
    "\n",
    "ข้อตกลงในการตั้งชื่อตัวแปร:\n",
    "- ตัวแปร Python ที่เก็บค่าอนุพันธ์ย่อย จะใช้รูปแบบการตั้งชื่อดังนี้  \n",
    " $\\frac{\\partial J(w,b)}{\\partial b}$  จะถูกตั้งชื่อเป็น dj_db\n",
    "- w.r.t คือ With Respect To (เทียบกับ) เช่น อนุพันธ์ย่อยของ J(wb) เทียบกับ b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.3\"></a>\n",
    "### compute_gradient\n",
    "<a name='ex-01'></a>\n",
    "`compute_gradient` ฟังก์ชันนี้ดำเนินการตามขั้นตอนที่ 4 และ 5 ด้านบน และส่งคืนค่า  \n",
    " $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. คำอธิบายในโค้ดบรรยายการดำเนินการต่างๆ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"./images/C1_W1_Lab03_lecture_slopes_v1.png\"   style=\"width:340px;\" > บทบรรยายอธิบายว่าการไล่ระดับ (gradient descent) ใช้อนุพันธ์ย่อยของต้นทุนเทียบกับพารามิเตอร์ที่จุดหนึ่งเพื่ออัปเดตพารามิเตอร์นั้น\n",
    "\n",
    "มาใช้ฟังก์ชัน compute_gradient ของเรากันเพื่อหาและพล็อตอนุพันธ์ย่อยบางส่วนของฟังก์ชันต้นทุนของเรามากกว่าหนึ่งพารามิเตอร์ $w_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ด้านซ้าย: แสดงกราฟของ $\\frac{\\partial J(w,b)}{\\partial w}$ หรือความชันของเส้นโค้งค่าใช้จ่ายเทียบกับ $w$ a ที่สามจุด  ทางด้านขวาของกราฟ อนุพันธ์เป็นบวก ในขณะที่ทางด้านซ้าย อนุพันธ์เป็นลบ เนื่องจากรูปร่าง \"ชาม\" ของเส้นโค้ง อนุพันธ์จะนำการไล่ระดับลงไปที่ด้านล่างซึ่งมีค่าอนุพันธ์เป็นศูนย์เสมอ\n",
    " \n",
    "ด้านซ้าย $b=100$ คงที่ การไล่ระดับจะใช้ทั้ง $\\frac{\\partial J(w,b)}{\\partial w}$ และ  $\\frac{\\partial J(w,b)}{\\partial b}$ เพื่อปรับปรัมพารามิเตอร์ \"กราฟลูกศร\" ทางด้านขวาแสดงวิธีการมองเห็นการไล่ระดับของทั้งสองพารามิเตอร์ ขนาดของลูกศรสะท้อนถึงขนาดของการไล่ระดับที่จุดนั้น ทิศทางและความชันของลูกศรสะท้อนถึงอัตราส่วนของ $\\frac{\\partial J(w,b)}{\\partial w}$ และ   $\\frac{\\partial J(w,b)}{\\partial b}$ ที่จุดนั้น\n",
    "\n",
    "หมายเหตุ: การไล่ระดับชี้ไป ห่างจาก จุดต่ำสุด  ตรวจสอบสมการ (3) ด้านบน การไล่ระดับที่ปรับขนาดแล้วจะถูก *ลบ* ออกจากค่าปัจจุบันของ `w` หรือ `b` ทำให้พารามิเตอร์เคลื่อนที่ไปในทิศทางที่ลดค่าใช้จ่าย\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.5\"></a>\n",
    "###  Gradient Descent\n",
    "ตอนนี้ที่เราคำนวณการไล่ระดับได้แล้ว  เราสามารถนำการไล่ระดับมาใช้ในฟังก์ชัน gradient_descent ตามสมการ (3) ด้านบน  รายละเอียดของการใช้งานมีการอธิบายในส่วนหมายเหตุ  ด้านล่างนี้ คุณจะใช้ฟังก์ชันนี้เพื่อหาค่าที่เหมาะสมที่สุดของ $w$ และ $b$ บนข้อมูลการฝึก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"./images/C1_W1_Lab03_lecture_learningrate_v1.png\"  style=\"width:340px; padding: 15px; \" > \n",
    "จากข้อมูลที่ให้มา สังเกตได้ว่ากระบวนการ Gradient Descent มีคุณสมบัติหลักดังนี้:\n",
    "\n",
    "- ค่าฟังก์ชันต้นทุน (Cost) ลดลงอย่างรวดเร็วในช่วงแรก: ตามที่อธิบายในสไลด์จากการบรรยาย \n",
    "- ค่าต้นทุนจะเริ่มสูงและลดลงอย่างรวดเร็ว อนุพันธ์ย่อย (Partial Derivatives) `dj_dw` และ `dj_db` ลดลงอย่างรวดเร็วในช่วงแรก: จากแผนภาพในสไลด์ เมื่อกระบวนการเข้าใกล้ \"จุดต่ำสุดของชาม\" (bottom of the bowl) อนุพันธ์ย่อยจะลดลงอย่างช้าลง เนื่องจากค่าของอนุพันธ์ย่อย ณ จุดนั้นมีค่าน้อยลง\n",
    "การเรียนรู้ (Learning Rate) alpha คงที่ แต่ความก้าวหน้าลดลง: \n",
    "- แม้ว่าอัตราการเรียนรู้จะคงที่ แต่ความก้าวหน้าในการปรับน้ำหนักและค่าตัดขวางจะลดลงตามเวลา เนื่องจากอนุพันธ์ย่อยลดลง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost กับการวนซ้ำของ gradient descent \n",
    "Cost กับการวนซ้ำของ gradient descent  เป็นตัววัดความก้าวหน้าที่มีประโยชน์ใน gradient descent. Cost ควรลดลงเสมอในการรันที่ประสบความสำเร็จ. การเปลี่ยนแปลงของ cost นั้นรวดเร็วมากในช่วงแรก จึงเป็นประโยชน์ในการพล็อตการลดลงเริ่มต้นในมาตราส่วนที่แตกต่างจากการลดลงสุดท้าย ในพล็อตด้านล่าง โปรดสังเกตมาตราส่วนของ cost บนแกนและขั้นตอนการวนซ้ำ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions (การคาดการณ์)\n",
    "ตอนนี้คุณได้ค้นพบค่าที่เหมาะสมที่สุดสำหรับพารามิเตอร์ $w$ และ $$ แล้ว คุณสามารถใช้โมเดลเพื่อทำนายมูลค่าบ้านโดยอิงจากพารามิเตอร์ที่เรียนรู้ของคุณได้แล้ว ตามที่คาดไว้ ค่าที่ทำนายได้นั้นใกล้เคียงกับค่าฝึกอบรมสำหรับบ้านหลังเดียวกัน นอกจากนี้ ค่าที่ไม่อยู่ในการทำนายยังสอดคล้องกับค่าที่คาดหวัง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.6\"></a>\n",
    "## การพล็อต\n",
    "คุณสามารถแสดงความคืบหน้าของการไล่ระดับสีในระหว่างการดำเนินการโดยการพล็อตค่า cost เหนือการทำซ้ำบนพล็อตคอนทัวร์ของค่า cost (w,b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ด้านบน แผนที่ระดับความสูงแสดงค่า $cost(w,b)$ ในช่วงค่า w และ b ที่หลากหลาย ระดับค่า cost จะแสดงด้วยวงแหวน เส้นทางการไล่ระดับความชันจะทับซ้อนกันโดยใช้ลูกศรสีแดง นี่คือสิ่งที่ควรสังเกต:\n",
    "\n",
    "- เส้นทางมีความก้าวหน้าอย่างต่อเนื่อง (ซ้ำซ้อน) ไปสู่เป้าหมาย\n",
    "- ขั้นตอนเริ่มต้นมีขนาดใหญ่กว่าขั้นตอนใกล้เป้าหมายมาก"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ซูมเข้าไป**\"เราจะเห็นขั้นตอนสุดท้ายของการไล่ระดับลง (gradient descent) สังเกตว่าระยะห่างระหว่างขั้นตอนจะลดลงเมื่อการไล่ระดับเข้าใกล้ศูนย์\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n",
    "            contours=[1,5,10,20],resolution=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.7.1\"></a>\n",
    "### การเพิ่ม Learning Rate\n",
    "\n",
    "<figure>\n",
    " <img align=\"left\", src=\"./images/C1_W1_Lab03_alpha_too_big_v1.png\"   style=\"width:340px;height:240px;\" >\n",
    "</figure>\n",
    "\n",
    "ในการบรรยาย มีการพูดคุยเกี่ยวกับค่าที่เหมาะสมของอัตราการเรียนรู้ α ในสมการ (3) ค่า  $\\alpha$ ที่สูงขึ้นจะทำให้การไล่ระดับลงมาบรรจบกับคำตอบได้เร็วขึ้น แต่ถ้าสูงเกินไป การไล่ระดับลงมาจะแยกออกไป\n",
    "\n",
    "ด้านบนคุณมีตัวอย่างของคำตอบที่บรรจบกันอย่างดี\n",
    "\n",
    "ลองเพิ่มค่า  $\\alpha$ และดูว่าจะเกิดอะไรขึ้น:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# set alpha to a large value\n",
    "iterations = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ด้านบน $w$ และ $b$ กำลังเด้งกลับไปกลับมาระหว่างบวกและลบโดยมีค่าสัมบูรณ์เพิ่มขึ้นในแต่ละรอบ นอกจากนี้ ในแต่ละรอบ  $\\frac{\\partial J(w,b)}{\\partial w}$\n",
    "เปลี่ยนเครื่องหมายและค่าใช้จ่ายเพิ่มขึ้นแทนที่จะลดลง นี่เป็นสัญญาณที่ชัดเจนว่า *อัตราการเรียนรู้สูงเกินไป *และโซลูชันกำลังเบี่ยงเบน\n",
    "\n",
    "มาลองดูภาพประกอบกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_divergence(p_hist, J_hist,x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ด้านบน กราฟทางซ้ายแสดงความคืบหน้าของ $w$ ในช่วงแรก ๆ ของการไล่ระดับลง $w$ สั่นจากบวกเป็นลบและต้นทุนเพิ่มขึ้นอย่างรวดเร็ว การไล่ระดับลงทำงานบนทั้ง $w$ และ $b$ พร้อมกัน ดังนั้นจึงต้องใช้แผนภูมิ 3 มิติทางขวาเพื่อภาพรวมที่สมบูรณ์"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ขอแสดงความยินดี!\n",
    "ในห้องปฏิบัติการนี้ คุณได้:\n",
    "\n",
    "- เจาะลึกรายละเอียดของการไล่ระดับสำหรับตัวแปรเดียว\n",
    "- พัฒนาโปรแกรมเพื่อคำนวณการไล่ระดับ\n",
    "- แสดงภาพว่าการไล่ระดับคืออะไร\n",
    "- เสร็จสิ้นโปรแกรมการไล่ระดับ\n",
    "- ใช้การไล่ระดับเพื่อหาพารามิเตอร์\n",
    "- ตรวจสอบผลกระทบของการปรับขนาดอัตราการเรียนรู้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
