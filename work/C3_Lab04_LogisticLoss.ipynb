{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ห้องปฏิบัติการเสริม: การถดถอยโลจิสติก (Logistic Regression) และฟังก์ชันการสูญเสียโลจิสติก (Logistic Loss)\n",
    "ในห้องปฏิบัติการนี้ (ไม่ให้คะแนน) คุณจะ:\n",
    "\n",
    "- สำรวจเหตุผลที่ฟังก์ชันการสูญเสียแบบกำลังสอง (squared error loss) ไม่เหมาะสำหรับการถดถอยโลจิสติก\n",
    "- สำรวจฟังก์ชันการสูญเสียโลจิสติก (logistic loss function)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "try:\n",
    "  %matplotlib widget\n",
    "  print(\"widget is already installed\")\n",
    "except:\n",
    "  print(\"widget is not been installed, install now..\")\n",
    "  !pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c374302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: import requsts and download from this github link : https://raw.githubusercontent.com/Smith-WeStrideTH/Machine_Learning_Course/main/work/deeplearning.mplstyle\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Machine_Learning_Course/main/work/deeplearning.mplstyle'\n",
    "url2 = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Machine_Learning_Course/main/work/plt_logistic_loss.py'\n",
    "url3 = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Machine_Learning_Course/main/work/lab_utils_common_c3.py'\n",
    "\n",
    "response = requests.get(url)\n",
    "with open('deeplearning.mplstyle', 'wb') as f:\n",
    "  f.write(response.content)\n",
    "\n",
    "response = requests.get(url2)\n",
    "with open('plt_logistic_loss.py', 'wb') as f:\n",
    "  f.write(response.content)\n",
    "\n",
    "response = requests.get(url3)\n",
    "with open('lab_utils_common.py', 'wb') as f:\n",
    "  f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6572b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n",
    "from plt_logistic_loss import soup_bowl, plt_logistic_squared_error\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bc6a2",
   "metadata": {},
   "source": [
    "## Squared error for logistic regression?\n",
    "<img align=\"left\" src=\"./images/C1_W3_SqErrorVsLogistic_v1.png\"     style=\" width:400px; padding: 10px; \" > การใช้ฟังก์ชันค่าใช้จ่ายแบบ Squared Error **ในการถดถอยเชิงเส้น (Linear Regression)**\n",
    "ในการถดถอยเชิงเส้น เราได้ใช้ **ฟังก์ชันค่าใช้จ่ายแบบ Squared Error (Squared Error Cost Function)**\n",
    "\n",
    "สมการของฟังก์ชันค่าใช้จ่ายแบบ Squared Error สำหรับตัวแปรเดียวคือ:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
    " \n",
    "โดยที่ \n",
    "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad53364",
   "metadata": {},
   "source": [
    "การลดค่าใช้จ่ายแบบ Squared Error มีคุณสมบัติที่ดีอย่างหนึ่งคือการไล่ตามอนุพันธ์ของค่าใช้จ่ายจะนำไปสู่ค่าใช้จ่ายที่ต่ำที่สุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e0758c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97852c78127f4107a949593bf65c928b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soup_bowl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4d249",
   "metadata": {},
   "source": [
    "ฟังก์ชันต้นทุนที่ใช้ได้ดีสำหรับการถดถอยเชิงเส้น (linear regression) ก็สามารถนำมาใช้สำหรับการถดถอยโลจิสติก (logistic regression) ได้เช่นกัน\n",
    "\n",
    "อย่างไรก็ตาม ดังที่สไลด์ก่อนหน้านี้ชี้ให้เห็นว่า  $f_{wb}(x)$   ในตอนนี้มีองค์ประกอบที่ไม่เป็นเชิงเส้น (non-linear) ซึ่งก็คือฟังก์ชันซิกมอยด์ (sigmoid function):  $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$.   ลองใช้ค่าความผิดพลาดกำลังสอง (squared error cost) กับตัวอย่างจากการทดลองก่อนหน้านี้ ซึ่งตอนนี้รวมฟังก์ชันซิกมอยด์ด้วย\n",
    "\n",
    "นี่คือข้อมูลการฝึกของเรา:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c9df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5331a6363c334eae83bc7baa6722c363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\n",
    "plt_simple_example(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d5225",
   "metadata": {},
   "source": [
    "ตอนนี้ มาสร้างแผนภาพพื้นผิว (surface plot) ของค่าใช้จ่ายโดยใช้ *squared error cost*:\n",
    "\n",
    "\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ \n",
    " \n",
    "โดยที่ \n",
    "  $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7def76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54143f99bd1c4908a3ff3f8ad3fe0cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt_logistic_squared_error(x_train,y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e78a3",
   "metadata": {},
   "source": [
    "แม้ว่ากราฟที่ได้จะน่าสนใจ แต่พื้นผิวของกราฟ (surface) นั้นไม่เรียบเท่า \"ชามซุป\" ที่ได้จากการถดถอยเชิงเส้น (linear regression)!\n",
    "\n",
    "    \n",
    "\n",
    "การถดถอยโลจิสติก (Logistic Regression) ต้องใช้ฟังก์ชันต้นทุน (Cost Function) ที่เหมาะสมกับลักษณะที่ไม่เป็นเชิงเส้น (non-linear) ของมัน ฟังก์ชันต้นทุนนี้เรียกว่า Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e6cc7",
   "metadata": {},
   "source": [
    "## Logistic Loss Function\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_a_v1.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_b_v1.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_c_v1.png\"     style=\" width:250px; padding: 2px; \" > "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f730f",
   "metadata": {},
   "source": [
    "การถดถอยโลจิสติก (Logistic Regression) ใช้ฟังก์ชันการสูญเสีย (loss function) ที่เหมาะสมสำหรับงานการจัดประเภท (categorization) โดยที่เป้าหมาย (target) มีค่า 0 หรือ 1 แทนที่จะเป็นตัวเลขใดๆ\n",
    "\n",
    ">**คำจำกัดความ:**   \n",
    "**Loss** คือการวัดความแตกต่างระหว่างตัวอย่างเดียวกับค่าเป้าหมายของมัน  \n",
    "**Cost** คือการวัดการสูญเสียทั้งหมดในชุดข้อมูลฝึก (training set)\n",
    "\n",
    "\n",
    "นิยามของฟังก์ชันการสูญเสีย:\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ คือค่าการสูญเสียสำหรับข้อมูลจุดเดียว ซึ่งมีนิยามดังนี้:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ คือการทำนายของโมเดล ขณะที่ $y^{(i)}$  คือค่าเป้าหมาย\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ โดยที่ฟังก์ชัน $g$ คือฟังก์ชันซิกมอยด์ (sigmoid function)\n",
    "\n",
    "ลักษณะเฉพาะของฟังก์ชันการสูญเสีย:\n",
    "\n",
    "- ฟังก์ชันการสูญเสียนี้มีลักษณะเฉพาะคือใช้เส้นโค้งสองเส้นแยกกัน เส้นหนึ่งสำหรับกรณีที่เป้าหมายเป็นศูนย์ $(y=0)$และอีกเส้นหนึ่งสำหรับกรณีที่เป้าหมายเป็นหนึ่ง $(y=1)$\n",
    "- เมื่อรวมกัน เส้นโค้งทั้งสองนี้จะแสดงพฤติกรรมที่เหมาะสมสำหรับฟังก์ชันการสูญเสีย คือ มีค่าเป็นศูนย์เมื่อการทำนายตรงกับเป้าหมาย และเพิ่มขึ้นอย่างรวดเร็วเมื่อการทำนายแตกต่างจากเป้าหมาย\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06df3867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f536f2edfc34f498fe2423ad75c99d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_two_logistic_loss_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60477d",
   "metadata": {},
   "source": [
    "เมื่อรวมเส้นโค้งทั้งสองแล้ว จะคล้ายกับเส้นโค้งกำลังสองของค่าใช้จ่าย (squared error loss)\n",
    "แกน X คือ $f_{\\mathbf{w},b}$ ซึ่งเป็นผลลัพธ์ของฟังก์ชันซิกมอยด์ (sigmoid)\n",
    "ผลลัพธ์ของฟังก์ชันซิกมอยด์จะอยู่ระหว่าง 0 ถึง 1 เท่านั้น\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2106a",
   "metadata": {},
   "source": [
    "การเขียนสมการ Loss Function ใหม่ให้ใช้งานง่ายขึ้น, สมการ Loss Function ด้านบนสามารถเขียนใหม่ได้ดังนี้:\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "สมการนี้ดูซับซ้อน แต่จะง่ายขึ้นเมื่อพิจารณาว่า  $y^{(i)}$ มีค่าได้เพียงสองค่า คือ 0 และ 1:  \n",
    "กรณี  $ y^{(i)} = 0$, เทอมทางซ้ายถูกตัดทิ้ง:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "กรณี  $ y^{(i)} = 1$, เทอมทางขวาถูกตัดทิ้ง:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ด้วยฟังก์ชันการสูญเสียแบบ Logistic (Logistic Loss Function) เราสามารถสร้างฟังก์ชันต้นทุนที่รวมการสูญเสียจากตัวอย่างทั้งหมดได้หัวข้อต่อไปใน Lab จะพูดถึงเรื่องนี้\n",
    "ในตอนนี้ มาดูตัวอย่างง่ายๆ ที่เราเคยพิจารณากันก่อนหน้านี้: กราฟค่าใช้จ่ายเทียบกับพารามิเตอร์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a967218",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "cst = plt_logistic_cost(x_train,y_train)\n",
    "cst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "กราฟนี้เหมาะสำหรับการไล่ระดับ (gradient descent)\n",
    "ข้อสังเกต:\n",
    "\n",
    "- กราฟนี้ไม่มีที่ราบสูง (plateaus) จุดต่ำสุดท้องถิ่น (local minima) หรือการขาดต่อเนื่อง (discontinuities)\n",
    "- แตกต่างจากกรณีของข้อผิดพลาดกำลังสอง (squared error): กราฟของข้อผิดพลาดกำลังสองจะมีรูปร่างคล้ายชาม (bowl)\n",
    "- การพล็อตทั้งค่าใช้จ่ายและล็อกของค่าใช้จ่าย: ช่วยให้เห็นได้ชัดเจนว่าเมื่อค่าใช้จ่ายมีขนาดเล็ก กราฟจะมีความลาดชันและยังคงลดลงต่อไป\n",
    "\n",
    "**หมายเหตุ:** คุณสามารถหมุนกราฟเหล่านี้ได้โดยใช้เมาส์ของคุณ\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ยินดีด้วย!\n",
    "คุณได้:\n",
    "- ตระหนักว่าฟังก์ชันการสูญเสียแบบกำลังสอง (squared error loss function) ไม่เหมาะสำหรับงานการจำแนกประเภท (classification tasks)\n",
    "- พัฒนาและตรวจสอบฟังก์ชันการสูญเสียแบบโลจิสติก (logistic loss function) ซึ่ง เหมาะ สำหรับงานการจำแนกประเภท\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
