{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ภารกิจเสริม: วิศวกรรมลักษณะข้อมูล (Feature Engineering) และการถดถอยพหุนาม (Polynomial Regression)\n",
    "![](./images/C1_W2_Lab07_FeatureEngLecture_v1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20efb09",
   "metadata": {},
   "source": [
    "## เป้าหมาย\n",
    "ในห้องปฏิบัติการนี้ คุณจะ:\n",
    "- สำรวจการออกแบบฟีเจอร์ (feature engineering) และการถดถอยพหุนาม (polynomial regression) ซึ่งช่วยให้คุณสามารถใช้เครื่องจักรของการถดถอยเชิงเส้นเพื่อปรับให้เข้ากับฟังก์ชันที่ซับซ้อนมาก ๆ แม้กระทั่งฟังก์ชันที่ไม่เชิงเส้นมาก ๆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6aa990",
   "metadata": {},
   "source": [
    "## เครื่องมือ\n",
    "คุณจะใช้ฟังก์ชันที่พัฒนาในแล็บก่อนหน้านี้ รวมถึง matplotlib และ NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: import requsts and download from this github link : https://raw.githubusercontent.com/Smith-WeStrideTH/Machine_Learning_Course/main/work/deeplearning.mplstyle\n",
    "\n",
    "import requests\n",
    "\n",
    "url2 = 'https://raw.githubusercontent.com/Smith-WeStrideTH/Machine_Learning_Course/main/work/lab_utils_multi.py'\n",
    "\n",
    "response = requests.get(url2)\n",
    "with open('lab_utils_multi.py', 'wb') as f:\n",
    "  f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import zscore_normalize_features, run_gradient_descent_feng\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082b07d",
   "metadata": {},
   "source": [
    "<a name='FeatureEng'></a>\n",
    "# การออกแบบคุณลักษณะ (Feature Engineering) และการถดถอยแบบพหุนาม (Polynomial Regression)\n",
    "\n",
    "การถดถอยเชิงเส้น (Linear Regression) แบบพื้นฐานช่วยสร้างโมเดลในรูปแบบต่อไปนี้:\n",
    "$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$ \n",
    "แต่ถ้าคุณลักษณะ (features) หรือข้อมูลของคุณไม่เป็นเชิงเส้น หรือเป็นการรวมกันของคุณลักษณะหลายตัวล่ะ? ตัวอย่างเช่น ราคาบ้านอาจไม่สัมพันธ์เชิงเส้นกับพื้นที่อยู่อาศัย แต่จะมีค่าใช้จ่ายเพิ่มขึ้นอย่างรวดเร็วสำหรับบ้านที่มีขนาดเล็กมากหรือขนาดใหญ่มาก ส่งผลให้เกิดเส้นโค้งดังภาพด้านบน\n",
    "เราจะใช้เครื่องมือของการถดถอยเชิงเส้นเพื่อปรับให้เข้ากับเส้นโค้งนี้ได้อย่างไร? จำไว้ว่า 'เครื่องมือ' ที่เรามีคือความสามารถในการปรับเปลี่ยนพารามิเตอร์  $\\mathbf{w}$, $\\mathbf{b}$ in (1)  ในสมการ (1) เพื่อ 'ปรับ' สมการให้เข้ากับข้อมูลการฝึก (training data) อย่างไรก็ตาม การปรับเปลี่ยน $\\mathbf{w}$,$\\mathbf{b}$  ในสมการ (1) เท่าใดก็ตาม จะไม่สามารถทำให้เข้ากับเส้นโค้งแบบไม่เชิงเส้นได้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd751a6",
   "metadata": {},
   "source": [
    "<a name='PolynomialFeatures'></a>\n",
    "## ฟีเจอร์แบบพหุนาม (Polynomial Features)\n",
    "\n",
    "ก่อนหน้านี้ เราได้พิจารณาสถานการณ์ที่ข้อมูลไม่ใช่เชิงเส้นตรง (non-linear)\n",
    "ลองใช้สิ่งที่เรารู้มาจนถึงตอนนี้เพื่อปรับโค้งแบบไม่ใช่เชิงเส้นตรงกัน\n",
    "เราจะเริ่มต้นด้วยฟังก์ชันกำลังสองง่ายๆ: $y = 1+x^2$\n",
    "\n",
    "คุณคุ้นเคยกับฟังก์ชันทั้งหมดที่เรากำลังใช้อยู่แล้ว คุณสามารถดูรายละเอียดเพิ่มเติมได้ในไฟล์  lab_utils.py [`np.c_[..]`](https://numpy.org/doc/stable/reference/generated/numpy.c_.html) เป็นฟังก์ชันใน NumPy ที่ใช้สำหรับการเชื่อมต่อ (concatenate) ข้อมูลตามแนวคอลัมน์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaed97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model_w,model_b = run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-2)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf6075",
   "metadata": {},
   "source": [
    "ดังที่คาดไว้ การใช้โมเดลเชิงเส้นตรงอาจไม่เหมาะสม สิ่งที่จำเป็นคือโมเดลที่มีรูปแบบคล้ายกับ $y= w_0x_0^2 + b$ หรือเรียกว่า โมเดลพหุนาม (polynomial feature)\n",
    "เพื่อให้บรรลุเป้าหมายนี้ คุณสามารถปรับเปลี่ยน ข้อมูลอินพุต เพื่อ ออกแบบ ฟีเจอร์ที่ต้องการได้\n",
    "หากคุณสลับข้อมูลต้นฉบับด้วยเวอร์ชันที่ยกกำลังสองของค่า $x$ คุณจะสามารถบรรลุ  $y= w_0x_0^2 + b$. ได้ ลองทำตามนี้ สลับ `X` เป็น  `X**2` ดังต่อไปนี้:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "\n",
    "# Engineer features \n",
    "X = x**2      #<-- added engineered feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25aa2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1)  #X should be a 2-D Matrix\n",
    "model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\")\n",
    "plt.plot(x, np.dot(X,model_w) + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865b719",
   "metadata": {},
   "source": [
    "กราฟที่ใกล้เคียงกับเป้าหมายอย่างมาก\n",
    "สังเกตค่าของ $w$ และ $b$ ที่พิมพ์ด้านบนของกราฟ: `w,b พบว่า gradient descent: w: [1.], b: 0.0490`. การไล่ระดับ (Gradient Descent) ได้ปรับค่าเริ่มต้นของ  $\\mathbf{w},b $ ให้เป็น (1.0,0.049) ซึ่งสอดคล้องกับโมเดล $y=1*x_0^2+0.049$, ใกล้เคียงกับเป้าหมาย $y=1*x_0^2+1$. หากคุณให้มันทำงานนานขึ้น อาจจะได้ผลลัพธ์ที่ดีกว่านี้"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207198f9",
   "metadata": {},
   "source": [
    "### เลือกฟีเจอร์ที่เหมาะสมสำหรับการสร้างโมเดล\n",
    "<a name='GDF'></a>\n",
    "ในตัวอย่างก่อนหน้า เราทราบว่าจำเป็นต้องใช้เทอม $x^2$ อย่างไรก็ตาม ในปัญหาจริง อาจไม่ชัดเจนว่าฟีเจอร์ใดจำเป็นต้องใช้บ้าง  เราสามารถทดลองเพิ่มฟีเจอร์ต่างๆ เพื่อหาฟีเจอร์ที่เหมาะสมที่สุด\n",
    "\n",
    "ตัวอย่าง: เราอาจลองใช้สมการต่อไปนี้แทน: $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ? \n",
    "\n",
    "ลองรันเซลล์ถัดไปเพื่อทดสอบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# engineer features .\n",
    "X = np.c_[x, x**2, x**3]   #<-- added engineered feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"x, x**2, x**3 features\")\n",
    "plt.plot(x, X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e75202",
   "metadata": {},
   "source": [
    "หมายเหตุเกี่ยวกับค่า  $\\mathbf{w}$, `[0.08 0.54 0.03]` และ b คือ  `0.0106`.สิ่งนี้หมายความว่าโมเดลหลังจากการฟิต (fitting) หรือการฝึก (training) คือ:\n",
    "$$ 0.08x + 0.54x^2 + 0.03x^3 + 0.0106 $$\n",
    "การไล่ระดับ (gradient descent) ได้เน้นให้ความสำคัญกับข้อมูลที่เหมาะสมที่สุดกับข้อมูล  $x^2$ โดยเพิ่มค่า $w_1$ เมื่อเทียบกับค่าอื่นๆ  Iหากคุณเรียกใช้การไล่ระดับเป็นเวลานานมาก มันจะลดผลกระทบของเทอมอื่นๆ ต่อไป\n",
    ">Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter\n",
    "\n",
    "ทบทวนแนวคิดนี้:\n",
    "- ค่าน้ำหนัก (weight) ที่มีค่าน้อยกว่า หมายถึงฟีเจอร์นั้นมีความสำคัญหรือนำไปสู่การคาดการณ์ที่ถูกต้องน้อยกว่า และในกรณีสุดขั้ว เมื่อค่าน้ำหนักใกล้ศูนย์หรือเท่ากับศูนย์ ฟีเจอร์นั้นจะไม่เป็นประโยชน์ในการปรับโมเดลให้เข้ากับข้อมูล\n",
    "- หลังจากการปรับโมเดล ค่าน้ำหนักที่เกี่ยวข้องกับฟีเจอร์ $x^2$ จะมีค่ามากขึ้นอย่างเห็นได้ชัดเมื่อเทียบกับค่าน้ำหนักของ  $x$ หรือ $x^3$ มีประโยชน์มากที่สุดในการปรับโมเดลให้เข้ากับข้อมูล\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62332a5e",
   "metadata": {},
   "source": [
    "### มุมมองทางเลือก\n",
    "ข้างต้น เราเลือกฟีเจอร์แบบพหุนาม (polynomial features) โดยพิจารณาจากความเหมาะสมกับข้อมูลเป้าหมาย วิธีการอีกอย่างหนึ่งในการคิดเกี่ยวกับเรื่องนี้คือการสังเกตว่าเรายังคงใช้การถดถอยเชิงเส้น (linear regression) หลังจากที่เราได้สร้างฟีเจอร์ใหม่แล้ว\n",
    "เมื่อพิจารณาถึงเรื่องนี้แล้ว ฟีเจอร์ที่ดีที่สุดจะสัมพันธ์เชิงเส้นกับเป้าหมาย (target) ซึ่งสามารถเข้าใจได้ดีที่สุดผ่านตัวอย่างต่อไปนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc09928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# engineer features .\n",
    "X = np.c_[x, x**2, x**3]   #<-- added engineered feature\n",
    "X_features = ['x','x^2','x^3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c051925",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X[:,i],y)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd6031",
   "metadata": {},
   "source": [
    "จากกราฟข้างต้น เราเห็นได้ชัดว่าคุณสมบัติ $x^2$ เมื่อจับคู่กับค่าเป้าหมาย $y$ มีความสัมพันธ์เชิงเส้น (linear relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781c323",
   "metadata": {},
   "source": [
    "### มาตรการปรับขนาด (Scaling features)\n",
    "ตามที่ได้กล่าวไปในแล็บก่อนหน้านี้ หากชุดข้อมูลของคุณมีคุณลักษณะ (features) ที่มีสเกลแตกต่างกันอย่างมาก ควรใช้เทคนิคการปรับขนาดคุณลักษณะ (feature scaling) เพื่อเร่งการไล่ระดับ (gradient descent)\n",
    "ในตัวอย่างข้างต้น มีตัวแปร $x$, $x^2$ และ $x^3$ ซึ่งจะมีสเกลที่แตกต่างกันอย่างมากตามธรรมชาติ มาลองใช้การแปลง Z-score เพื่อปรับขนาดตัวอย่างของเราให้เหมาะสมกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8919405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0,20,1)\n",
    "X = np.c_[x, x**2, x**3]\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X,axis=0)}\")\n",
    "\n",
    "# add mean_normalization \n",
    "X = zscore_normalize_features(X)     \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b841f",
   "metadata": {},
   "source": [
    "ตอนนี้เราสามารถลองใช้ค่า alpha ที่เข้มข้นกว่านี้:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de833e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,20,1)\n",
    "y = x**2\n",
    "\n",
    "X = np.c_[x, x**2, x**3]\n",
    "X = zscore_normalize_features(X) \n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee7213",
   "metadata": {},
   "source": [
    "การปรับสเกลฟีเจอร์ช่วยให้การไล่ระดับบรรจบกันได้เร็วขึ้น\n",
    "โปรดสังเกตค่าของ  $\\mathbf{w}$ อีกครั้ง พจน์ $w_1$ ซึ่งเป็นพจน์ $x^2$  มีความสำคัญมากที่สุด การไล่ระดับได้ลดพจน์  $x^3$ ลงไปเกือบหมดแล้ว"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20583ce0",
   "metadata": {},
   "source": [
    "### ฟังก์ชันที่ซับซ้อน (Complex Functions)\n",
    "ด้วยการปรับแต่งคุณสมบัติ (feature engineering) สามารถสร้างแบบจำลองแม้แต่ฟังก์ชันที่ซับซ้อนได้:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60968f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,20,1)\n",
    "y = np.cos(x/2)\n",
    "\n",
    "X = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\n",
    "X = zscore_normalize_features(X) \n",
    "\n",
    "model_w,model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha = 1e-1)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64100fb1",
   "metadata": {},
   "source": [
    "## ยินดีด้วย!\n",
    "ใน Lab นี้ คุณได้เรียนรู้:\n",
    "\n",
    "- วิธีที่การถดถอยเชิงเส้น (linear regression) - สามารถสร้างแบบจำลองฟังก์ชันที่ซับซ้อนแม้กระทั่งแบบที่ไม่เชิงเส้นอย่างมาก โดยใช้การวิศวกรรมลักษณะ (feature engineering)ความสำคัญของการปรับขนาดลักษณะ (feature scaling) เมื่อทำการวิศวกรรมลักษณะ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
